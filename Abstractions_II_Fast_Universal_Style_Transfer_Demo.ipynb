{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Abstractions II - Fast Universal Style Transfer Demo.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "KlQK7Shab3Hp",
        "oaCeVZPi2rri",
        "qgxot8pSW6Kj",
        "OfHpGO8c63jv",
        "_AzmblSe8wru",
        "O73IkOTmv33N",
        "NROrTlKwwPLe",
        "2NL2QB4ts0A_",
        "ILasPqzKwU9S",
        "gqA-p5m_UPGO",
        "703BA4idiHV3",
        "SA5ZGUlUjLSx",
        "E0l7X88kdBwj",
        "OYU_UuqIeQjZ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCY0VS9F_jpL",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title #### Copyright Â© 2019 Evan Davis\n",
        "\n",
        "#@markdown evan@skimai.com / [@eridgd](http://github.com/eridgd/)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "G-YtyBbx_S2Z",
        "colab": {}
      },
      "source": [
        "#@title #### Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "# <table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "#   <td>\n",
        "#     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/models/blob/master/research/nst_blogpost/4_Neural_Style_Transfer_with_Eager_Execution.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "#   </td>\n",
        "#   <td>\n",
        "#     <a target=\"_blank\" href=\"https://github.com/tensorflow/models/blob/master/research/nst_blogpost/4_Neural_Style_Transfer_with_Eager_Execution.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "#   </td>\n",
        "# </table>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TR-DY5AK-P8d"
      },
      "source": [
        "# Artistic Style Transfer with Convolutional Neural Networks\n",
        "\n",
        "## Abstractions II &#151; August 23, 2019 &#151; Pittsburgh, PA\n",
        "\n",
        "![](https://github.com/eridgd/ArtNet-WCT-TF2/raw/master/samples/abstractions_header.gif)\n",
        "\n",
        "This notebook accompanies my [talk on Neural Artistic Style Transfer](https://docs.google.com/presentation/d/1mj_dyBkSylKMhQKzyGu7_oHpr6IbpaGXGuJDnQgrQGE/edit?usp=sharing) for [Abstractions II](https://abstractions.io/) and contains TensorFlow 2.0 implementations for five papers:\n",
        "\n",
        "* [Fast Universal Style Transfer for Artistic and Photorealistic Rendering](https://arxiv.org/abs/1907.03118) An et al, 2019\n",
        "* [A Closed-form Solution to Universal Style Transfer](https://arxiv.org/abs/1906.00668) Lu et al, 2019\n",
        "* [Universal Style Transfer via Feature Transforms](https://arxiv.org/abs/1705.08086) Li et al, 2017\n",
        "* [Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization](https://arxiv.org/abs/1703.06868) Huang and Belongie, 2017\n",
        "* [Fast Patch-based Style Transfer of Arbitrary Style](https://arxiv.org/abs/1612.04337) Chen and Schmidt, 2016"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJi_bQ8tRnxv",
        "colab_type": "text"
      },
      "source": [
        "# TO RUN THIS NOTEBOOK\n",
        "\n",
        "This notebook is intended to demonstrate style transfer in real-time and requires a webcam. It may or may not work on a mobile device.\n",
        "\n",
        "Just a few quick steps to get started:\n",
        " * Log in to a Google account.\n",
        " * **_File -> Save a copy in Drive..._**  -- Save a copy of this notebook in your GDrive.\n",
        " * **_Runtime -> Run all_** -- Initiates a free GPU Cloud VM, downloads models, and starts webcam demo.\n",
        " * <a href=\"#scrollTo=9len-gzKr5hz\">Scroll to the bottom</a> to see the live stylized output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MtGoR1VyxyvL"
      },
      "source": [
        "# Setup TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0C78ZMMa2BK",
        "colab_type": "text"
      },
      "source": [
        "## Make sure we're using a GPU instance\n",
        "\n",
        "If **`nvidia-smi`** shows no GPU you'll need to change the runtime type:\n",
        "* **_Runtime -> Change runtime type_**\n",
        "* Under `Hardware accelerator` choose `GPU` and save.\n",
        "* Run the notebook again on the new instance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVPdfdXZa4bJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KlQK7Shab3Hp"
      },
      "source": [
        "## Install TensorFlow 2.0 Beta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kd4IwEShUx2B",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0-beta1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2rDmGg5ZzkS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yUd6jcKikP2M",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "assert tf.executing_eagerly()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oaCeVZPi2rri"
      },
      "source": [
        "# Downloads\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgxot8pSW6Kj",
        "colab_type": "text"
      },
      "source": [
        "### VGG19 Weights\n",
        "\n",
        "We'll being using a variant of the \"VGG19\" convolutional network as our image encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ui3YKin2eqxY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VGG_PATH = 'vgg_normalised.t7'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z10vHzSNW6Kl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -c -O {VGG_PATH} \"https://www.dropbox.com/s/kh8izr3fkvhitfn/vgg_normalised.t7?dl=1\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l3kL4Lzw9rsH"
      },
      "source": [
        "### Pre-trained Decoder weights\n",
        "\n",
        "This is an 'ArtNet'-style decoder architecture that was separately trained to invert VGG features to reconstruct the original input image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5M7N_k5ev5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "WEIGHTS_PATH = 'artnet_decoder_full_nnupsample_40kunlabeled_tv0.keras'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jOH43vLV85t2",
        "colab": {}
      },
      "source": [
        "!wget -c -O {WEIGHTS_PATH} https://www.dropbox.com/s/f992fv5zv4ao0dw/artnet_decoder_full_nnupsample_40kunlabeled_tv0.keras?dl=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DQSs2azsVUEf"
      },
      "source": [
        "### Sample style images\n",
        "\n",
        "For style images we'll use a sample of the WikiArt dataset obtained from https://github.com/cs-chan/ArtGAN/tree/master/WikiArt%20Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGY7fVXJMYzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -c -O wikiart_samples_abstractions.tar.gz https://www.dropbox.com/s/h1ngp8ajthpitku/wikiart_samples_abstractions.tar.gz?dl=1\n",
        "\n",
        "!tar xfz wikiart_samples_abstractions.tar.gz\n",
        "\n",
        "from pathlib import Path\n",
        "style_image_paths = [str(f) for f in Path('wikiart_samples_abstractions').glob('**/*.jpg')]\n",
        "len(style_image_paths)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OfHpGO8c63jv"
      },
      "source": [
        "# Setup Utilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GXZ1lTrKdc8F"
      },
      "source": [
        "## Image utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kCLNUsyfdgS2",
        "colab": {}
      },
      "source": [
        "def resize_image_keep_aspect(image, lo_dim):\n",
        "    shape = tf.shape(image)\n",
        "    width, height = shape[0], shape[1]\n",
        "    min_ = tf.minimum(width, height)\n",
        "    ratio = tf.cast(min_, tf.float32) / tf.constant(lo_dim, dtype=tf.float32)\n",
        "    new_width = tf.cast(tf.cast(width, tf.float32) / ratio, tf.int32)\n",
        "    new_height = tf.cast(tf.cast(height, tf.float32) / ratio, tf.int32)\n",
        "    return tf.image.resize(image, [new_width, new_height])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYKh-XjSwuaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crop_center(image):\n",
        "    h, w = image.shape[-3], image.shape[-2]\n",
        "    if h > w:\n",
        "        cropped_image = tf.image.crop_to_bounding_box(image, (h - w) // 2, 0, w, w)\n",
        "    else:\n",
        "        cropped_image = tf.image.crop_to_bounding_box(image, 0, (w - h) // 2, h, h)\n",
        "    return cropped_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XvOTks98dgS7",
        "colab": {}
      },
      "source": [
        "def preprocess_image(image, size):\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    # image = tf.cast(image, tf.float32)\n",
        "    image = resize_image_keep_aspect(image, size)\n",
        "    image = tf.image.random_crop(image, (size,size,3))\n",
        "    # image = crop_center(image)\n",
        "    image /= 255.0  # normalize to [0,1] range\n",
        "    return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lZJJAGHtdgS9",
        "colab": {}
      },
      "source": [
        "def load_and_preprocess_image(path, size):\n",
        "    image = tf.io.read_file(path)\n",
        "    return preprocess_image(image, size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m-9121yBek2N",
        "colab": {}
      },
      "source": [
        "import PIL\n",
        "import IPython.display as ipydisplay\n",
        "\n",
        "def plot_image(image):\n",
        "    if len(image.shape) == 4:\n",
        "        image = image[0]\n",
        "    image *= 255\n",
        "    ipydisplay.display(PIL.Image.fromarray((image).numpy().astype('uint8')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-N90pZfbBPQb",
        "colab": {}
      },
      "source": [
        "def deconv_output_length(input_length, filter_size, padding, stride):\n",
        "  \"\"\"Determines output length of a transposed convolution given input length.\n",
        "  Arguments:\n",
        "      input_length: integer.\n",
        "      filter_size: integer.\n",
        "      padding: one of \"same\", \"valid\", \"full\".\n",
        "      stride: integer.\n",
        "  Returns:\n",
        "      The output length (integer).\n",
        "  From: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/utils.py#L159\n",
        "  \"\"\"\n",
        "  if input_length is None:\n",
        "    return None\n",
        "  input_length *= stride\n",
        "  if padding == 'valid':\n",
        "    input_length += max(filter_size - stride, 0)\n",
        "  elif padding == 'full':\n",
        "    input_length -= (stride + filter_size - 2)\n",
        "  return input_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_AzmblSe8wru"
      },
      "source": [
        "## Define Feature Transform Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O73IkOTmv33N"
      },
      "source": [
        "### White-Color Transform (WCT)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OX-VCkzW8yV0",
        "colab": {}
      },
      "source": [
        "def wct_tf(content, style, alpha, eps=1e-9):\n",
        "    '''TensorFlow version of Whiten-Color Transform\n",
        "       Assume that content/style encodings have shape 1xHxWxC\n",
        "\n",
        "       See p.4 of the Universal Style Transfer paper for corresponding equations:\n",
        "       https://arxiv.org/pdf/1705.08086.pdf\n",
        "    '''\n",
        "    # Remove batch dim and reorder to CxHxW\n",
        "    content_t = tf.transpose(tf.squeeze(content), (2, 0, 1))\n",
        "    style_t = tf.transpose(tf.squeeze(style), (2, 0, 1))\n",
        "\n",
        "    Cc, Hc, Wc = tf.unstack(tf.shape(content_t))\n",
        "    Cs, Hs, Ws = tf.unstack(tf.shape(style_t))\n",
        "\n",
        "    # CxHxW -> CxH*W\n",
        "    content_flat = tf.reshape(content_t, (Cc, Hc*Wc))\n",
        "    style_flat = tf.reshape(style_t, (Cs, Hs*Ws))\n",
        "\n",
        "    # Content covariance\n",
        "    mc = tf.reduce_mean(content_flat, axis=1, keepdims=True)\n",
        "    fc = content_flat - mc\n",
        "    fcfc = fc @ tf.transpose(fc) / tf.cast((Hc*Wc - 1), tf.float32) + tf.eye(Cc)*eps\n",
        "\n",
        "    # Style covariance\n",
        "    ms = tf.reduce_mean(style_flat, axis=1, keepdims=True)\n",
        "    fs = style_flat - ms\n",
        "    fsfs = fs @ tf.transpose(fs) / tf.cast((Hs*Ws - 1), tf.float32) + tf.eye(Cs)*eps\n",
        "\n",
        "    # tf.linalg.svd is slower on GPU, see https://github.com/tensorflow/tensorflow/issues/13603\n",
        "    with tf.device('/cpu:0'):  \n",
        "        Sc, Uc, _ = tf.linalg.svd(fcfc)\n",
        "        Ss, Us, _ = tf.linalg.svd(fsfs)\n",
        "\n",
        "    # # Uncomment to use PyTorch to compute SVD on CPU. Embarrasingly faster than TF.\n",
        "    # import torch\n",
        "    # Uc, Sc, _ = torch.svd(torch.from_numpy(fcfc.numpy()))\n",
        "    # Us, Ss, _ = torch.svd(torch.from_numpy(fsfs.numpy()))\n",
        "    # Uc, Sc, Us, Ss = Uc.numpy(), Sc.numpy(), Us.numpy(), Ss.numpy()\n",
        "\n",
        "    # Filter small singular values\n",
        "    k_c = tf.reduce_sum(tf.cast(tf.greater(Sc, 1e-5), tf.int32))\n",
        "    k_s = tf.reduce_sum(tf.cast(tf.greater(Ss, 1e-5), tf.int32))\n",
        "\n",
        "    # Whiten content feature\n",
        "    Dc_inv_sqrt = tf.linalg.diag(tf.pow(Sc[:k_c], -0.5))\n",
        "    fc_hat = Uc[:,:k_c] @ Dc_inv_sqrt @ tf.transpose(Uc[:,:k_c]) @ fc\n",
        "\n",
        "    # Color content with style\n",
        "    Ds_sqrt = tf.linalg.diag(tf.pow(Ss[:k_s], 0.5))\n",
        "    fcs_hat = Us[:,:k_s] @ Ds_sqrt @ tf.transpose(Us[:,:k_s]) @ fc_hat\n",
        "\n",
        "    # Re-center with mean of style\n",
        "    fcs_hat = fcs_hat + ms\n",
        "\n",
        "    # Blend whiten-colored feature with original content feature\n",
        "    blended = alpha * fcs_hat + (1 - alpha) * (fc + mc)\n",
        "\n",
        "    # CxH*W -> CxHxW\n",
        "    blended = tf.reshape(blended, (Cc,Hc,Wc))\n",
        "    # CxHxW -> 1xHxWxC\n",
        "    blended = tf.expand_dims(tf.transpose(blended, (1,2,0)), 0)\n",
        "\n",
        "    return blended"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NROrTlKwwPLe"
      },
      "source": [
        "### Adaptive Instance Normalization (AdaIN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ywy3tkA0wOnt",
        "colab": {}
      },
      "source": [
        "def adain(content_features, style_features, alpha, epsilon=1e-5):\n",
        "    '''\n",
        "    Borrowed from https://github.com/jonrei/tf-AdaIN\n",
        "    Normalizes the `content_features` with scaling and offset from `style_features`.\n",
        "    See \"5. Adaptive Instance Normalization\" in https://arxiv.org/abs/1703.06868 for details.\n",
        "    '''\n",
        "    style_mean, style_variance = tf.nn.moments(style_features, [1,2], keepdims=True)\n",
        "    content_mean, content_variance = tf.nn.moments(content_features, [1,2], keepdims=True)\n",
        "    normalized_content_features = tf.nn.batch_normalization(content_features, content_mean,\n",
        "                                                            content_variance, style_mean, \n",
        "                                                            tf.sqrt(style_variance), epsilon)\n",
        "    normalized_content_features = alpha * normalized_content_features + (1 - alpha) * content_features\n",
        "    return normalized_content_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NL2QB4ts0A_",
        "colab_type": "text"
      },
      "source": [
        "### \"Closed Form\" Transform (Optimal Transport)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c08E0JtRHzti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import torch\n",
        "\n",
        "def optimal_tf(content, style, alpha, eps=1e-4):\n",
        "    '''TensorFlow version of Optimal Transport transform from \"A Closed-form Solution to Universal Style Transfer\"\n",
        "       Paper: https://arxiv.org/abs/1906.00668 \n",
        "       Author's Torch code: https://github.com/lu-m13/OptimalStyleTransfer/blob/master/optimal.lua\n",
        "       PyTorch version: https://github.com/sunshineatnoon/PytorchWCT/blob/a7a8e29b0561c231c8a1ffcdd952d954231d18e6/util.py\n",
        "    '''\n",
        "    # Remove batch dim and reorder to CxHxW\n",
        "    content_t = tf.transpose(tf.squeeze(content), (2, 0, 1))\n",
        "    style_t = tf.transpose(tf.squeeze(style), (2, 0, 1))\n",
        "\n",
        "    Cc, Hc, Wc = tf.unstack(tf.shape(content_t))\n",
        "    Cs, Hs, Ws = tf.unstack(tf.shape(style_t))\n",
        "\n",
        "    # CxHxW -> CxH*W\n",
        "    content_flat = tf.reshape(content_t, (Cc, Hc*Wc))\n",
        "    style_flat = tf.reshape(style_t, (Cs, Hs*Ws))\n",
        "\n",
        "    # Content covariance\n",
        "    mc = tf.reduce_mean(content_flat, axis=1, keepdims=True)\n",
        "    fc = content_flat - mc\n",
        "    fcfc = fc @ tf.transpose(fc) / tf.cast((Hc*Wc - 1), tf.float32) + tf.eye(Cc)*eps\n",
        "\n",
        "    # Style covariance\n",
        "    ms = tf.reduce_mean(style_flat, axis=1, keepdims=True)\n",
        "    fs = style_flat - ms\n",
        "    fsfs = fs @ tf.transpose(fs) / tf.cast((Hs*Ws - 1), tf.float32) + tf.eye(Cs)*eps\n",
        "\n",
        "    # tf.linalg.svd is slower on GPU, see https://github.com/tensorflow/tensorflow/issues/13603\n",
        "    with tf.device('/cpu:0'):  \n",
        "        Sc, Uc, _ = tf.linalg.svd(fcfc)\n",
        "    \n",
        "    # # Uncomment to use PyTorch to compute SVD on CPU. Embarrasingly faster than TF.\n",
        "    # Uc, Sc, _ = torch.svd(torch.from_numpy(fcfc.numpy()), some=False)\n",
        "    # Uc, Sc = Uc.numpy(), Sc.numpy()\n",
        "\n",
        "    # Filter small singular values\n",
        "    k_c = tf.reduce_sum(tf.cast(tf.greater(Sc, 1e-5), tf.int32))\n",
        "\n",
        "    Dc_sqrt     = tf.linalg.diag(tf.pow(Sc[:k_c], 0.5))\n",
        "    Dc_inv_sqrt = tf.linalg.diag(tf.pow(Sc[:k_c], -0.5))\n",
        "    \n",
        "    cF_sqrt = Uc[:,:k_c] @ Dc_sqrt @ tf.transpose(Uc[:,:k_c])\n",
        "    cF_inv_sqrt = Uc[:,:k_c] @ Dc_inv_sqrt @ tf.transpose(Uc[:,:k_c])\n",
        "\n",
        "    middle_matrix = cF_sqrt @ fsfs @ cF_sqrt\n",
        "    with tf.device('/cpu:0'):\n",
        "        Sm, Um, _ = tf.linalg.svd(middle_matrix)\n",
        "\n",
        "    # # Uncomment to use PyTorch to compute SVD\n",
        "    # Um, Sm, _ = torch.svd(torch.from_numpy(middle_matrix.numpy()), some=False)\n",
        "    # Um, Sm = Um.numpy(), Sm.numpy()\n",
        "\n",
        "    k_m = tf.reduce_sum(tf.cast(tf.greater(Sm, 1e-5), tf.int32))\n",
        "    Dm_sqrt = tf.linalg.diag(tf.pow(Sm[:k_m], 0.5))\n",
        "    middle_matrix_sqrt = Um[:,:k_m] @ Dm_sqrt @ tf.transpose(Um[:, :k_m])\n",
        "    \n",
        "    # Apply transformation matrix to centered content feature\n",
        "    transform_matrix = cF_inv_sqrt @ middle_matrix_sqrt @ cF_inv_sqrt\n",
        "    fcs_hat = transform_matrix @ fc\n",
        "\n",
        "    # Re-center with mean of style\n",
        "    fcs_hat = fcs_hat + ms\n",
        "\n",
        "    # Blend whiten-colored feature with original content feature\n",
        "    blended = alpha * fcs_hat + (1 - alpha) * (fc + mc)\n",
        "\n",
        "    # CxH*W -> CxHxW\n",
        "    blended = tf.reshape(blended, (Cc,Hc,Wc))\n",
        "    # CxHxW -> 1xHxWxC\n",
        "    blended = tf.expand_dims(tf.transpose(blended, (1,2,0)), 0)\n",
        "\n",
        "    return blended"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ILasPqzKwU9S"
      },
      "source": [
        "### Style-Swap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zQovGEKLwUUY",
        "colab": {}
      },
      "source": [
        "def style_swap(content, style, patch_size, stride):\n",
        "    '''Efficiently swap content feature patches with nearest-neighbor style patches\n",
        "       Original paper: https://arxiv.org/abs/1612.04337\n",
        "       Adapted from: https://github.com/rtqichen/style-swap/blob/master/lib/NonparametricPatchAutoencoderFactory.lua\n",
        "    '''\n",
        "    nC = tf.shape(style)[-1]  # Num channels of input content feature and style-swapped output\n",
        "\n",
        "    ### Extract patches from style image that will be used for conv/deconv layers\n",
        "    style_patches = tf.image.extract_patches(style, [1,patch_size,patch_size,1], [1,stride,stride,1], [1,1,1,1], 'VALID')\n",
        "    before_reshape = tf.shape(style_patches)  # NxRowsxColsxPatch_size*Patch_size*nC\n",
        "    style_patches = tf.reshape(style_patches, [before_reshape[1]*before_reshape[2],patch_size,patch_size,nC])\n",
        "    style_patches = tf.transpose(style_patches, [1,2,3,0])  # Patch_sizexPatch_sizexIn_CxOut_c\n",
        "\n",
        "    # Normalize each style patch\n",
        "    style_patches_norm = tf.nn.l2_normalize(style_patches, axis=3)\n",
        "\n",
        "    # Compute cross-correlation/nearest neighbors of patches by using style patches as conv filters\n",
        "    ss_enc = tf.nn.conv2d(content,\n",
        "                          style_patches_norm,\n",
        "                          [1,stride,stride,1],\n",
        "                          'VALID')\n",
        "\n",
        "    # For each spatial position find index of max along channel/patch dim  \n",
        "    ss_argmax = tf.argmax(ss_enc, axis=3)\n",
        "    encC = tf.shape(ss_enc)[-1]  # Num channels in intermediate conv output, same as # of patches\n",
        "    \n",
        "    # One-hot encode argmax with same size as ss_enc, with 1's in max channel idx for each spatial pos\n",
        "    ss_oh = tf.one_hot(ss_argmax, encC, 1., 0., 3)\n",
        "\n",
        "    # Calc size of transposed conv out\n",
        "    deconv_out_H = deconv_output_length(tf.shape(ss_oh)[1], patch_size, 'valid', stride)\n",
        "    deconv_out_W = deconv_output_length(tf.shape(ss_oh)[2], patch_size, 'valid', stride)\n",
        "    deconv_out_shape = tf.stack([1,deconv_out_H,deconv_out_W,nC])\n",
        "\n",
        "    # Deconv back to original content size with highest matching (unnormalized) style patch swapped in for each content patch\n",
        "    ss_dec = tf.nn.conv2d_transpose(ss_oh,\n",
        "                                    style_patches,\n",
        "                                    deconv_out_shape,\n",
        "                                    [1,stride,stride,1],\n",
        "                                    'VALID')\n",
        "\n",
        "    ### Interpolate to average overlapping patch locations\n",
        "    ss_oh_sum = tf.reduce_sum(ss_oh, axis=3, keepdims=True)\n",
        "\n",
        "    filter_ones = tf.ones([patch_size,patch_size,1,1], dtype=tf.float32)\n",
        "    \n",
        "    deconv_out_shape = tf.stack([1,deconv_out_H,deconv_out_W,1])  # Same spatial size as ss_dec with 1 channel\n",
        "\n",
        "    counting = tf.nn.conv2d_transpose(ss_oh_sum,\n",
        "                                         filter_ones,\n",
        "                                         deconv_out_shape,\n",
        "                                         [1,stride,stride,1],\n",
        "                                         'VALID')\n",
        "\n",
        "    counting = tf.tile(counting, [1,1,1,nC])  # Repeat along channel dim to make same size as ss_dec\n",
        "\n",
        "    interpolated_dec = tf.divide(ss_dec, counting)\n",
        "\n",
        "    return interpolated_dec\n",
        "\n",
        "\n",
        "def wct_style_swap(content, style, alpha, ss_blend, patch_size=3, stride=1, eps=1e-9):\n",
        "    '''Modified Whiten-Color Transform that performs style swap on whitened content/style encodings before coloring\n",
        "       Assume that content/style encodings have shape 1xHxWxC\n",
        "    '''\n",
        "    beta=0.5\n",
        "    content_t = tf.transpose(tf.squeeze(content), (2, 0, 1))\n",
        "    style_t = tf.transpose(tf.squeeze(style), (2, 0, 1))\n",
        "\n",
        "    Cc, Hc, Wc = tf.unstack(tf.shape(content_t))\n",
        "    Cs, Hs, Ws = tf.unstack(tf.shape(style_t))\n",
        "\n",
        "    # CxHxW -> CxH*W\n",
        "    content_flat = tf.reshape(content_t, (Cc, Hc*Wc))\n",
        "    style_flat = tf.reshape(style_t, (Cs, Hs*Ws))\n",
        "\n",
        "    # Content covariance\n",
        "    mc = tf.reduce_mean(content_flat, axis=1, keepdims=True)\n",
        "    fc = content_flat - mc\n",
        "    fcfc = fc @ tf.transpose(fc) / tf.cast((Hc*Wc - 1), tf.float32) + tf.eye(Cc)*eps\t\n",
        "\n",
        "    # Style covariance\n",
        "    ms = tf.reduce_mean(style_flat, axis=1, keepdims=True)\n",
        "    fs = style_flat - ms\n",
        "    fsfs = fs @ tf.transpose(fs) / tf.cast((Hs*Ws - 1), tf.float32) + tf.eye(Cs)*eps\t\n",
        "\n",
        "    # tf.linalg.svd is slower on GPU, see https://github.com/tensorflow/tensorflow/issues/13603\n",
        "    with tf.device('/cpu:0'):  \n",
        "        Sc, Uc, _ = tf.linalg.svd(fcfc)\n",
        "        Ss, Us, _ = tf.linalg.svd(fsfs)\n",
        "\n",
        "    k_c = tf.reduce_sum(tf.cast(tf.greater(Sc, 1e-5), tf.int32))\n",
        "    k_s = tf.reduce_sum(tf.cast(tf.greater(Ss, 1e-5), tf.int32))\n",
        "\n",
        "    ### Whiten content feature\n",
        "    Dc = tf.linalg.diag(tf.pow(Sc[:k_c], -0.5))\n",
        "\n",
        "    fc_hat = Uc[:,:k_c] @ Dc @ tf.transpose(Uc[:,:k_c]) @ fc\t\t\n",
        "    # fc_hat = tf.matmul(tf.matmul(tf.matmul(Uc[:,:k_c], Dc), Uc[:,:k_c], transpose_b=True), fc)\n",
        "\n",
        "    # Reshape before passing to style swap, CxH*W -> 1xHxWxC\n",
        "    whiten_content = tf.expand_dims(tf.transpose(tf.reshape(fc_hat, [Cc,Hc,Wc]), [1,2,0]), 0)\n",
        "\n",
        "    ### Whiten style before swapping\n",
        "    Ds_inv_sqrt = tf.linalg.diag(tf.pow(Ss[:k_s], -0.5))\n",
        "    whiten_style = Us[:,:k_s] @ Ds_inv_sqrt @ tf.transpose(Us[:,:k_s]) @ fs\n",
        "    # whiten_style = tf.matmul(tf.matmul(tf.matmul(Us[:,:k_s], Ds_inv_sqrt), Us[:,:k_s], transpose_b=True), fs)\n",
        "    # Reshape before passing to style swap, CxH*W -> 1xHxWxC\n",
        "    whiten_style = tf.expand_dims(tf.transpose(tf.reshape(whiten_style, [Cs,Hs,Ws]), [1,2,0]), 0)\n",
        "\n",
        "    ### Style swap whitened encodings\n",
        "    ss_feature = style_swap(whiten_content, whiten_style, patch_size, stride)\n",
        "    # # HxWxC -> CxH*W\n",
        "    # ss_feature = tf.transpose(tf.reshape(ss_feature, [Hc*Wc,Cc]), [1,0])\n",
        "\n",
        "    Wc_ss_blended = ss_blend * ss_feature + (1 - ss_blend) * whiten_content[0]\n",
        "    # # HxWxC -> CxH*W\n",
        "    Wc_ss_blended = tf.transpose(tf.reshape(Wc_ss_blended, [Hc*Wc,Cc]), [1,0])\n",
        "\n",
        "    ### Color style-swapped encoding with style \n",
        "    Ds_sqrt = tf.linalg.diag(tf.pow(Ss[:k_s], 0.5))\n",
        "    fcs_hat = Us[:,:k_s] @ Ds_sqrt @ tf.transpose(Us[:,:k_s]) @ Wc_ss_blended\n",
        "    # fcs_hat = tf.matmul(tf.matmul(tf.matmul(Us[:,:k_s], Ds_sqrt), Us[:,:k_s], transpose_b=True), ss_feature)\n",
        "    fcs_hat = fcs_hat + ms\n",
        "\n",
        "    ### Blend style-swapped & colored encoding with original content encoding\n",
        "    blended = alpha * fcs_hat + (1 - alpha) * (fc + mc)\n",
        "    # CxH*W -> CxHxW\n",
        "    blended = tf.reshape(blended, (Cc,Hc,Wc))\n",
        "    # CxHxW -> 1xHxWxC\n",
        "    blended = tf.expand_dims(tf.transpose(blended, (1,2,0)), 0)\n",
        "\n",
        "    return blended"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gqA-p5m_UPGO"
      },
      "source": [
        "# Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pYOGbUBdUPGO",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "\n",
        "from tensorflow.keras.layers import Input, UpSampling2D, MaxPooling2D, Conv2D, Activation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "703BA4idiHV3"
      },
      "source": [
        "## VGG19 from vgg_normalised.t7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIjnH4qAy5P4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Borrow a modified version of torchfile to read VGG weight file\n",
        "!wget --quiet -c https://raw.githubusercontent.com/eridgd/WCT-TF/master/torchfile.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKzUD9deW6L7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchfile\n",
        "\n",
        "def vgg_from_t7(t7_file, output_layers=['relu1_1','relu2_1','relu3_1','relu4_1','relu5_1'], trainable=False):\n",
        "    '''Extract VGG layers from a Torch .t7 model into a Keras model\n",
        "       e.g. vgg = vgg_from_t7('vgg_normalised.t7', target_layer='relu4_1')\n",
        "       Adapted from https://github.com/jonrei/tf-AdaIN/blob/master/AdaIN.py\n",
        "       Converted caffe->t7 from https://github.com/xunhuang1995/AdaIN-style\n",
        "    '''\n",
        "    t7 = torchfile.load(t7_file, force_8bytes_long=True)\n",
        "    \n",
        "    inp = Input(shape=(None, None, 3), name='vgg_input')\n",
        "    x = inp\n",
        "    \n",
        "    outputs = []\n",
        "    for idx,module in enumerate(t7.modules):\n",
        "        name = module.name.decode() if module.name is not None else None\n",
        "        \n",
        "        if idx == 0:\n",
        "            name = 'preprocess'  # VGG 1st layer preprocesses with a 1x1 conv to multiply by 255 and subtract BGR mean as bias\n",
        "\n",
        "        if module._typename == b'nn.SpatialReflectionPadding':\n",
        "            continue  # Use 'same' zero padding instead of reflection pad\n",
        "        elif module._typename == b'nn.SpatialConvolution':\n",
        "            filters = module.nOutputPlane\n",
        "            kernel_size = module.kH\n",
        "            weight = module.weight.transpose([2,3,1,0])\n",
        "            bias = module.bias\n",
        "            x = Conv2D(filters, kernel_size, padding='same', activation=None, name=name,\n",
        "                        kernel_initializer=tf.constant_initializer(weight),\n",
        "                        bias_initializer=tf.constant_initializer(bias),\n",
        "                        trainable=trainable)(x)\n",
        "        elif module._typename == b'nn.ReLU':\n",
        "            x = Activation('relu', name=name)(x)\n",
        "        elif module._typename == b'nn.SpatialMaxPooling':\n",
        "            x = MaxPooling2D(padding='same', name=name)(x)\n",
        "        else:\n",
        "            raise NotImplementedError(module._typename)\n",
        "            \n",
        "        if name in output_layers:\n",
        "            outputs.append(x)\n",
        "\n",
        "        if name == output_layers[-1]:\n",
        "            break\n",
        "    \n",
        "    model = tf.keras.Model(inputs=inp, outputs=outputs)\n",
        "\n",
        "    model.trainable = trainable\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YnQduD1SUPGY",
        "colab": {}
      },
      "source": [
        "class VGG19(tf.keras.Model):\n",
        "    def __init__(self, vgg_path=VGG_PATH):\n",
        "        super(VGG19, self).__init__()\n",
        "        self.vgg_tiers = vgg_from_t7(vgg_path, \n",
        "                             output_layers=['relu1_1','relu2_1','relu3_1','relu4_1','relu5_1'], \n",
        "                             trainable=False)\n",
        "        \n",
        "    def call(self, x):\n",
        "        return self.vgg_tiers(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SA5ZGUlUjLSx"
      },
      "source": [
        "## Decoder\n",
        "\n",
        "This is the only model that is trained. At inference time it allows feature transforms to be applied at multiple decoder layers to achieve stylization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rDYVfUuPiQXu"
      },
      "source": [
        "### Pyramid Fusion Conv layer to combine VGG features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiHC4eqvW6MG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PyramidFuseConv(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(PyramidFuseConv, self).__init__()\n",
        "        self.downsample_layers = []\n",
        "        for psize in [16, 8, 4, 2, 1]:\n",
        "            self.downsample_layers.append(Conv2D(filters=32, \n",
        "                                                 kernel_size=psize, \n",
        "                                                 strides=psize, \n",
        "                                                 padding='same',\n",
        "                                                 activation='relu'))\n",
        "        self.fuse_conv = Conv2D(filters=512, kernel_size=1, activation='relu', padding='same', \n",
        "                                name='pyramid_fuse_conv')\n",
        "    \n",
        "    def call(self, vgg_feats):\n",
        "        downsampled_feats = []\n",
        "        for feat, layer in zip(vgg_feats, self.downsample_layers):\n",
        "            feat = layer(feat)\n",
        "            downsampled_feats.append(feat)\n",
        "        feats_concat = tf.concat(downsampled_feats, axis=-1)\n",
        "        return self.fuse_conv(feats_concat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0l7X88kdBwj",
        "colab_type": "text"
      },
      "source": [
        "### ArtNet with Pyramid fuse layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4H6I-B27UPGe",
        "colab": {}
      },
      "source": [
        "class ArtNetDecoder(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(ArtNetDecoder, self).__init__()\n",
        "        self.pyramid_fuse_conv = PyramidFuseConv()  # Downsample & fuse intermediate VGG features\n",
        "        \n",
        "        self.decoder_tiers = {\n",
        "            5: [                                                                        #  HxW  / InC->OutC                                     \n",
        "                Conv2D(filters=512, kernel_size=3, padding='same', activation='relu'),  # 32x32 / 512->512\n",
        "                UpSampling2D(),                                                         # 32x32 -> 64x64\n",
        "                Conv2D(filters=512, kernel_size=3, padding='same', activation='relu'),  # 64x64 / 512->512\n",
        "                Conv2D(filters=512, kernel_size=3, padding='same', activation='relu'),  # 64x64 / 512->512\n",
        "                Conv2D(filters=512, kernel_size=3, padding='same', activation='relu')], # 64x64 / 512->512\n",
        "            4: [\n",
        "                Conv2D(filters=256, kernel_size=3, padding='same', activation='relu'),  # 64x64 / 512->256\n",
        "                UpSampling2D(),                                                         # 64x64 -> 128x128\n",
        "                Conv2D(filters=256, kernel_size=3, padding='same', activation='relu'),  # 128x128 / 256->256\n",
        "                Conv2D(filters=256, kernel_size=3, padding='same', activation='relu'),  # 128x128 / 256->256\n",
        "                Conv2D(filters=256, kernel_size=3, padding='same', activation='relu')], # 128x128 / 256->256\n",
        "            3: [\n",
        "                Conv2D(filters=128, kernel_size=3, padding='same', activation='relu'),  # 128x128 / 256->128\n",
        "                UpSampling2D(),                                                         # 128x128 -> 256x256\n",
        "                Conv2D(filters=128, kernel_size=3, padding='same', activation='relu')], # 256x256 / 128->128\n",
        "            2: [\n",
        "                Conv2D(filters=64,  kernel_size=3, padding='same', activation='relu'),  # 256x256 / 128->64\n",
        "                UpSampling2D()],                                                        # 256x256 -> 512x512\n",
        "            1: [\n",
        "                Conv2D(filters=64,  kernel_size=3, padding='same', activation='relu')]  # 512x512 / 64->64\n",
        "        }\n",
        "        \n",
        "        self.out = Conv2D(filters=3, kernel_size=3, padding='same', activation=None)    # 512x512 / 64->3\n",
        "    \n",
        "    def call(self, vgg_feats, return_tiers=None, style_decoder_feats=None, transform=wct_tf, alpha=1.,\n",
        "             swap_tiers=None, ss_blend=1., ss_patch_size=3, ss_stride=1, clip=False):\n",
        "        x = self.pyramid_fuse_conv(vgg_feats)\n",
        "\n",
        "        if return_tiers:\n",
        "            tier_ins = []\n",
        "\n",
        "        for i, tier in self.decoder_tiers.items():\n",
        "            if return_tiers and i in return_tiers:\n",
        "                tier_ins.append(x)\n",
        "                if i == min(return_tiers):\n",
        "                    return tier_ins\n",
        "\n",
        "            # Optionally apply WCT or AdaIN transform before decoding\n",
        "            # This is only used at inference time\n",
        "            if style_decoder_feats is not None and i in style_decoder_feats:  \n",
        "                if swap_tiers and i in swap_tiers:   # Style-swap + WCT\n",
        "                    x = wct_style_swap(x, style_decoder_feats[i], alpha=alpha, ss_blend=ss_blend, \n",
        "                                       patch_size=ss_patch_size, stride=ss_stride)\n",
        "                else:                                # WCT/AdaIN/OT\n",
        "                    x = transform(x, style_decoder_feats[i], alpha=alpha)\n",
        "\n",
        "            for layer in tier:\n",
        "                x = layer(x)\n",
        "\n",
        "        x = self.out(x)\n",
        "        \n",
        "        if clip:\n",
        "            x = tf.clip_by_value(x, 0, 1)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OYU_UuqIeQjZ"
      },
      "source": [
        "## Autoencoder with VGG Encoder and ArtNet Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_XXWLxd2enCG",
        "colab": {}
      },
      "source": [
        "encoder = VGG19()          # VGG19 weights are loaded automatically\n",
        "decoder = ArtNetDecoder()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZiO4LCCvea9D"
      },
      "source": [
        "### Load trained decoder weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XblzhdGvmx0z",
        "colab": {}
      },
      "source": [
        "# Loading weights for a subclassed model requires data to be run through it first. \n",
        "# See https://www.tensorflow.org/beta/guide/keras/saving_and_serializing#saving_subclassed_models\n",
        "_ = decoder(encoder(tf.ones([1,128,128,3])))  \n",
        "\n",
        "# Now we can load the weights.\n",
        "decoder.load_weights(WEIGHTS_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W9Mg-YUYlcBl"
      },
      "source": [
        "### Define stylization procedure\n",
        "\n",
        "`@tf.function` invokes AutoGraph to compile the model to a graph. See https://www.tensorflow.org/beta/guide/autograph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tDNsuLq8BPRz",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def extract_style_feats(encoder, decoder, style_img, tiers=[5,4,3]):\n",
        "    style_feats = encoder(style_img)\n",
        "    style_feats = decoder(style_feats, return_tiers=tiers)\n",
        "    return dict(zip(tiers, style_feats))\n",
        "\n",
        "@tf.function\n",
        "def stylize(encoder, decoder, content_img, style_img=None, style_feats=None, \n",
        "            tiers=[5,4,3], transform=wct_tf, alpha=0.6,\n",
        "            swap_tiers=None, ss_blend=0.6, ss_patch_size=3, ss_stride=1):\n",
        "    if style_feats is None:\n",
        "        style_feats = extract_style_feats(encoder, decoder, style_img, tiers)\n",
        "\n",
        "    inp = decoder(encoder(content_img),\n",
        "                    style_decoder_feats=style_feats,\n",
        "                    transform=transform,\n",
        "                    alpha=alpha,\n",
        "                    swap_tiers=swap_tiers, ss_blend=ss_blend, ss_patch_size=ss_patch_size, ss_stride=ss_stride,\n",
        "                    clip=True)\n",
        "    return inp   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9len-gzKr5hz"
      },
      "source": [
        "# Webcam Demo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_OMhtcfnBPSj",
        "colab": {}
      },
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def setup_stream():\n",
        "  js = Javascript('''\n",
        "    async function setupStream() {\n",
        "      const div = document.createElement('div');\n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'none';\n",
        "      stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      div.style.visibility='hidden' \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "      // google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "      canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "  }\n",
        "  ''')\n",
        "  display(js)\n",
        "  eval_js('setupStream()')\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0, video.videoWidth, video.videoHeight);\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  return binary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1y50UTYvIIDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import io\n",
        "import IPython.display as ipydisplay\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "def stylize_webcam(style_image_size, randomize_every, tiers, alpha, transform, passes, \n",
        "                   swap_tiers, ss_blend, ss_patch_size, ss_stride):\n",
        "    setup_stream()\n",
        "    disp_result = ipydisplay.display('', display_id='stylization')\n",
        "    disp_text = ipydisplay.display('', display_id='time')\n",
        "\n",
        "    i = -1\n",
        "    while True:\n",
        "        i += 1\n",
        "        try: \n",
        "            binary = take_photo()\n",
        "            content_img = Image.open(io.BytesIO(binary))\n",
        "            content_img = np.array(content_img)[None, ...].astype('float32') / 255\n",
        "\n",
        "            if i % randomize_every == 0:\n",
        "                style_height = content_img.shape[1]\n",
        "                style_path = random.choice(style_image_paths)\n",
        "                # style_path = 'sullivan.jpg'\n",
        "                disp_text.update(f\"Loading style from {style_path}\")\n",
        "                try:\n",
        "                    style_img = load_and_preprocess_image(style_path, style_image_size)  # Img is center-cropped square\n",
        "                    style_img = style_img[None, ...]  # Expand with batch dim\n",
        "                    style_feats_dict = extract_style_feats(encoder, decoder, style_img, tiers)\n",
        "                except Exception as e:  # tf.image.random_crop sometimes has issues\n",
        "                    print(e)\n",
        "                    i -= 1\n",
        "                    continue\n",
        "                # if style_img.shape[0] > style_image_size:\n",
        "                    # print(\"Resizing from \", style_img.shape, \"to\", style_image_size)\n",
        "                # style_img = tf.image.resize(style_img, [style_image_size, style_image_size])\n",
        "                # Pre-calculate the style image decoder features\n",
        "                # style_img = style_img[None, ...]  # Expand with batch dim\n",
        "                # style_feats_dict = extract_style_feats(encoder, decoder, style_img, tiers)\n",
        "\n",
        "                style_img_np = tf.image.resize(style_img[0], (content_img.shape[1],  content_img.shape[1])).numpy()\n",
        "            \n",
        "            s = time.time()\n",
        "            result = content_img\n",
        "            for _ in range(passes):\n",
        "                result = stylize(encoder, decoder,\n",
        "                            result, \n",
        "                            # style_img,\n",
        "                            style_feats=style_feats_dict, \n",
        "                            transform=transform, \n",
        "                            tiers=tiers,\n",
        "                            alpha=alpha,\n",
        "                            swap_tiers=swap_tiers, ss_blend=ss_blend, ss_patch_size=ss_patch_size, ss_stride=ss_stride)\n",
        "                \n",
        "            disp_text.update(f\"Frame {i} stylized in {time.time() - s}\")\n",
        "\n",
        "            output_img = np.hstack([result[0].numpy(), \n",
        "                                    np.zeros([style_height, 3, 3]), \n",
        "                                    style_img_np])\n",
        "            output_img = np.uint8(output_img * 255)\n",
        "            \n",
        "            # Convert to jpeg before displaying to speed things up\n",
        "            # See https://medium.com/@kostal91/displaying-real-time-webcam-stream-in-ipython-at-relatively-high-framerate-8e67428ac522\n",
        "            f = io.BytesIO()\n",
        "            Image.fromarray(output_img).save(f, 'jpeg')\n",
        "            disp_result.update(ipydisplay.Image(data=f.getvalue()))\n",
        "        #     eval_js('''stream.getVideoTracks()[0].stop();''')\n",
        "        except Exception as err:\n",
        "            # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "            # grant the page permission to access it.\n",
        "            print(str(err))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8bVs_v_FnBk",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@markdown **Stylization Settings for WCT / AdaIN / Optimal Transport**  \n",
        "\n",
        "#@markdown â¬ Press â¶ï¸ to start webcam stylization demo\n",
        "\n",
        "style_image_size = 512 #@param {type:\"slider\", min:128, max:2048, step:32}\n",
        "\n",
        "randomize_style_every = 8 #@param {type:\"integer\"}\n",
        "\n",
        "tiers = 5,4,3,2 #@param {type:\"raw\"}\n",
        "tiers = [int(t) for t in tiers]\n",
        "\n",
        "alpha = 0.85 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "\n",
        "transform = \"WCT\" #@param [\"WCT\", \"AdaIN\", \"Optimal Transport\"] {allow-input: false}\n",
        "if transform == 'WCT':\n",
        "    transform = wct_tf\n",
        "elif transform == 'Optimal Transport':\n",
        "    transform = optimal_tf\n",
        "else:\n",
        "    transform = adain\n",
        "\n",
        "passes = 1 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown **Style-Swap Settings**\n",
        "style_swap_on = True #@param {type:\"boolean\"}\n",
        "style_swap_tiers = 5,  #@param {type:\"raw\"}\n",
        "style_swap_tiers = [int(t) for t in style_swap_tiers]\n",
        "if style_swap_on is False:\n",
        "    style_swap_tiers = None\n",
        "\n",
        "style_swap_blend = 0.6 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "style_swap_patch_size = 3 #@param {type: \"integer\"}\n",
        "style_swap_stride = 1 #@param {type: \"integer\"}\n",
        "\n",
        "# multi_level_transform = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "stylize_webcam(style_image_size=style_image_size,\n",
        "               randomize_every=randomize_style_every, \n",
        "               tiers=tiers, \n",
        "               alpha=alpha,\n",
        "               transform=transform,\n",
        "               swap_tiers=style_swap_tiers,\n",
        "               ss_blend=style_swap_blend,\n",
        "               ss_patch_size=style_swap_patch_size,\n",
        "               ss_stride=style_swap_stride,\n",
        "               passes=passes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uX5MtZeBbdCC",
        "colab_type": "text"
      },
      "source": [
        "# Style Swap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AjBgti1eEqZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -c -O sullivan.jpg https://www.dropbox.com/s/rdzkzye8iihgtim/sullivan.jpg?dl=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EjWNiKi8h9hg",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import io\n",
        "import IPython.display as ipydisplay\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "def stylize_webcam(style_image_size, randomize_every, tiers, alpha, transform, passes, \n",
        "                   swap_tiers, ss_blend, ss_patch_size, ss_stride):\n",
        "    setup_stream()\n",
        "    disp_result = ipydisplay.display('', display_id='stylization')\n",
        "    disp_text = ipydisplay.display('', display_id='time')\n",
        "\n",
        "    style_img = tf.io.read_file('sullivan.jpg')\n",
        "    style_img = tf.image.decode_jpeg(style_img, channels=3)\n",
        "    style_img = resize_image_keep_aspect(style_img, style_image_size)\n",
        "    # style_img = tf.image.resize(style_img, [style_image_size, style_image_size])\n",
        "    style_img = tf.cast(style_img, tf.float32)\n",
        "    style_img = crop_center(style_img)\n",
        "    style_img /= 255.0  # normalize to [0,1] range\n",
        "    style_img = style_img[None, ...]  # Expand with batch dim\n",
        "    style_feats_dict = extract_style_feats(encoder, decoder, style_img, tiers)\n",
        "\n",
        "    i = -1\n",
        "    while True:\n",
        "        i += 1\n",
        "        try: \n",
        "            binary = take_photo()\n",
        "            content_img = Image.open(io.BytesIO(binary))\n",
        "            content_img = np.array(content_img)[None, ...].astype('float32') / 255\n",
        "\n",
        "            if i % randomize_every == 0:\n",
        "                style_height = content_img.shape[1]\n",
        "                # style_path = random.choice(style_image_paths)\n",
        "                style_path = 'sullivan.jpg'\n",
        "                # disp_text.update(f\"Loading style from {style_path}\")\n",
        "\n",
        "                style_img_np = tf.image.resize(style_img[0], (content_img.shape[1],  content_img.shape[1])).numpy()\n",
        "            \n",
        "            s = time.time()\n",
        "            result = content_img\n",
        "            for _ in range(passes):\n",
        "                result = stylize(encoder, decoder,\n",
        "                            result, \n",
        "                            # style_img,\n",
        "                            style_feats=style_feats_dict, \n",
        "                            transform=transform, \n",
        "                            tiers=tiers,\n",
        "                            alpha=alpha,\n",
        "                            swap_tiers=swap_tiers, ss_blend=ss_blend, ss_patch_size=ss_patch_size, ss_stride=ss_stride)\n",
        "                \n",
        "            disp_text.update(f\"Frame {i} stylized in {time.time() - s}\")\n",
        "\n",
        "            output_img = np.hstack([result[0].numpy(), \n",
        "                                    np.zeros([style_height, 3, 3]), \n",
        "                                    style_img_np])\n",
        "            output_img = np.uint8(output_img * 255)\n",
        "            \n",
        "            # Convert to jpeg before displaying to speed things up\n",
        "            # See https://medium.com/@kostal91/displaying-real-time-webcam-stream-in-ipython-at-relatively-high-framerate-8e67428ac522\n",
        "            f = io.BytesIO()\n",
        "            Image.fromarray(output_img).save(f, 'jpeg')\n",
        "            disp_result.update(ipydisplay.Image(data=f.getvalue()))\n",
        "        #     eval_js('''stream.getVideoTracks()[0].stop();''')\n",
        "        except Exception as err:\n",
        "            # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "            # grant the page permission to access it.\n",
        "            print(str(err))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJrsHVNqiOdj",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@markdown **Stylization Settings for WCT / AdaIN / Optimal Transport**  \n",
        "\n",
        "#@markdown â¬ Press â¶ï¸ to start webcam stylization demo\n",
        "\n",
        "style_image_size = 512 #@param {type:\"slider\", min:128, max:2048, step:32}\n",
        "\n",
        "randomize_style_every = 80 #@param {type:\"integer\"}\n",
        "\n",
        "tiers = 5,4,3,2 #@param {type:\"raw\"}\n",
        "tiers = [int(t) for t in tiers]\n",
        "\n",
        "alpha = 0.85 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "\n",
        "transform = \"WCT\" #@param [\"WCT\", \"AdaIN\", \"Optimal Transport\"] {allow-input: false}\n",
        "if transform == 'WCT':\n",
        "    transform = wct_tf\n",
        "elif transform == 'Optimal Transport':\n",
        "    transform = optimal_tf\n",
        "else:\n",
        "    transform = adain\n",
        "\n",
        "passes = 1 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown **Style-Swap Settings**\n",
        "style_swap_on = True #@param {type:\"boolean\"}\n",
        "style_swap_tiers = 5,  #@param {type:\"raw\"}\n",
        "style_swap_tiers = [int(t) for t in style_swap_tiers]\n",
        "if style_swap_on is False:\n",
        "    style_swap_tiers = None\n",
        "\n",
        "style_swap_blend = 1 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "style_swap_patch_size = 3 #@param {type: \"integer\"}\n",
        "style_swap_stride = 1 #@param {type: \"integer\"}\n",
        "\n",
        "# multi_level_transform = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "stylize_webcam(style_image_size=style_image_size,\n",
        "               randomize_every=randomize_style_every, \n",
        "               tiers=tiers, \n",
        "               alpha=alpha,\n",
        "               transform=transform,\n",
        "               swap_tiers=style_swap_tiers,\n",
        "               ss_blend=style_swap_blend,\n",
        "               ss_patch_size=style_swap_patch_size,\n",
        "               ss_stride=style_swap_stride,\n",
        "               passes=passes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAqcDqMmiPDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}